{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jdc\n",
    "import matplotlib.pyplot as plt\n",
    "from plotutil_partII import plotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "    - In Section 2.1, implement the helper function sigmoid \n",
    "    - In Section 2.2, implement the helper function normalize \n",
    "            (Attention: when you call it, DON'T use self.normalize becuase it is not a part of the LogisticRegression class)\n",
    "    - In Section 2.3, define the LogisticRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sigmoid Function ###\n",
    "\n",
    "Define a helper function 1: $sigmoid(Z) = \\frac{1}{1 + e^{-Z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Feature Scaling ###\n",
    "Define helper function 2 -- features normalization:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    '''\n",
    "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    # set keepdims=True to avoid rank-1 array\n",
    "    ### START YOUR CODE HERE ### \n",
    "    # calculate mean (1 line of code)\n",
    "    mean = \n",
    "    # calculate standard deviation (1 line of code)\n",
    "    std = \n",
    "    # normalize the matrix based on mean and std\n",
    "    matrix = \n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Logistic Regress Class ###\n",
    "You will create a neural network class - LogisticRegression:\n",
    "    - initialize parameters, such as weights, learning rate, etc.\n",
    "    - implement the gredient descent algorithm\n",
    "    - implement the predict function to make predictions for new data sets\n",
    "    - implement the normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, num_of_features=1, learning_rate=0.1, lmd = 1):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
    "\n",
    "        Argument:\n",
    "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
    "\n",
    "        Returns:\n",
    "        W -- initialized vector of shape (num_of_features, 1)\n",
    "        w_0 -- initialized scalar (corresponds to the bias)\n",
    "        \"\"\"\n",
    "        # n is the number of features\n",
    "        self.n = num_of_features\n",
    "        # alpha is the learning rate\n",
    "        self.alpha = learning_rate\n",
    "        # lambda is the parameter for regularization\n",
    "        self.lmd = lmd\n",
    "        \n",
    "        ### START YOUR CODE HERE ### \n",
    "        #initialize self.W and self.w_0 to be 0's\n",
    "        self.W = \n",
    "        self.w_0 = \n",
    "        ### YOUR CODE ENDS ###\n",
    "        assert(self.W.shape == (self.n, 1))\n",
    "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X with its shape as (n, m)\n",
    "- You compute  $$h_{W}(X) = a = \\sigma(w^T X + w_{0}) = \\frac{1}{1 + e^{-(w^T x + w_{0})}}\\tag{1}$$\n",
    "- You calculate the loss function:  $$L(W) = \\frac{1}{m} \\sum_{i=1}^{m}- y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)}) + \\frac{1}{2m} \\sum_{j=1}^{n}w_{j}^{2}\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using (pay attention to regularization): \n",
    "\n",
    "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)}) + \\frac{\\lambda}{m} * w_{j}  \\tag{3}$$\n",
    "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * dw_{j}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LogisticRegression\n",
    "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
    "    \"\"\"\n",
    "    This function implements the Gradient Descent Algorithm\n",
    "    Arguments:\n",
    "    X -- training data matrix: each column is a training example. \n",
    "            The number of columns is equal to the number of training examples\n",
    "    Y -- true \"label\" vector: shape (1, m)\n",
    "    epochs --\n",
    "\n",
    "    Return:\n",
    "    params -- dictionary containing weights\n",
    "    losses -- loss values of every 100 epochs\n",
    "    grads -- dictionary containing dw and dw_0\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        ### START YOUR CODE HERE ### \n",
    "        # Calculate the hypothesis outputs Y_hat (≈ 2 lines of code)\n",
    "        Z = \n",
    "        Y_hat = \n",
    "        # Calculate loss (≈ 1 line of code) (regularization happens here!!)\n",
    "        loss = \n",
    "        \n",
    "        # Calculate the gredients for W (regularization happens here!!)\n",
    "        dw = \n",
    "        # Calculate the gredients for w0 \n",
    "        dw_0 = \n",
    "\n",
    "        # Weight updates\n",
    "        self.W = \n",
    "        self.w_0 = \n",
    "        ### YOUR CODE ENDS ###\n",
    "\n",
    "        if((i % 100) == 0):\n",
    "            losses.append(loss)\n",
    "             # Print the cost every 100 training examples\n",
    "            if print_loss:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"W\": self.W,\n",
    "        \"w_0\": self.w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "\n",
    "    return params, grads, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = \\sigma(W^T * X + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LogisticRegression\n",
    "def predict(self, X):\n",
    "    '''\n",
    "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (n x m)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    #X = normalize(X)\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "\n",
    "    # Compute the actual values\n",
    "    ### START YOUR CODE HERE ### \n",
    "    Z_prediction = \n",
    "    A = \n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0, i] <= :\n",
    "            \n",
    "        else:\n",
    "            Y_prediction[0, i] = \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, lmd = 1, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    lmd -- lambda that is used for regularization\n",
    "    print_loss -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    num_of_features = X_train.shape[0]\n",
    "    model = LogisticRegression(num_of_features, learning_rate, lmd)\n",
    "    \n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
    "    parameters, grads, losses = \n",
    "    ### YOUR CODE ENDS ###\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = \n",
    "    Y_prediction_train = \n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    W = parameters['W']\n",
    "    w_0 = parameters['w_0']\n",
    "    print(\"W is \" + str(W))\n",
    "    print(\"w_0 is \" + str(w_0))\n",
    "    \n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs,\n",
    "        \"model\": model}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEGCAYAAABLgMOSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5SkdX3n8fdnRjimFQeRURHobvRMVBQkzCyKGoWgBkh0dM/mLKSDuKvby0bUya4meGY34MmOh5goalbNGdQNZjpw3PUCKIo4sut6QRkIzAwSYGR7hhECo+KgiwmX+e4fz1NSXV3VXZfnWs/ndU6dqnouVb+qrq5v/X7f30URgZmZ2aBWlF0AMzOrJwcQMzMbigOImZkNxQHEzMyG4gBiZmZDeVLZBSjS4YcfHtPT02UXw8ysVm666aYfR8Tqzu2NCiDT09Ns27at7GKYmdWKpN3dtrsJy8zMhuIAYmZmQ3EAMTOzoTiAmJnZUBxAzMxsKA4g1mhzO+aY/vA0K963gukPTzO3Y67sIpnVRqO68Zq1m9sxx+zVszz86MMA7N6/m9mrZwGYOW6mzKKZ1YJrINZYG7du/FXwaHn40YfZuHVjSSUyqxcHEGusPfv3DLS97txcZ1lzALHGmlw1OdD2Oms11+3ev5sgftVc5yBio3AAscbadNomJg6aWLBt4qAJNp22qaQS5cfNdZYHBxBrrJnjZtj8+s1MrZpCiKlVU2x+/eaxTKA3rbnOiuFeWNZoM8fNjGXA6DS5apLd+xfPhzeOzXVWHNdAzBqgSc11VhwHELMGaFJznRVHEVF2GQqzbt268HogZmaDkXRTRKzr3O4aiJmZDaXUACLp05IekLSzx35J+qikXZK2Szqxbd/pku5I911QXKnNzAzKr4H8DXD6EvvPANakl1ngEwCSVgIfS/cfC5wt6dhcS2pjwaOxzbJTajfeiPimpOklDlkPfCaSRM0Nkg6VdAQwDeyKiLsBJF2RHvuDfEtsdebJE82yVXYNZDlHAve03d+bbuu1fRFJs5K2Sdq2b9++3Apq1efR2GbZqnoAUZdtscT2xRsjNkfEuohYt3r16kwLZ9mam4PpaVixIrmey7h1yaOxzbJV9QCyFzi67f5RwL1LbLeampuD2VnYvRsikuvZ2WyDSJMmTzQrQtUDyFXAm9PeWC8D9kfEfcCNwBpJx0g6GDgrPdZqauNGeHhh6xIPP5xsz4pHY5tlq+xuvJcD3wWeL2mvpLdKOk/Seekh1wB3A7uAS4E/BIiIx4DzgWuB24HPRsRthb8Ay8yeHq1IvbYPo86jsd17zKrII9FtJHM75ti4dSN79u9hctUkm07bNNQX8vR00mzVaWoK5udHLmbhsnpfWo/V3nsMkppTXYKf1Z9HolvmslykaNMmmFjYusTERLK9brJevMm9x6yqHEAaJsumkCy/2GZmYPPmpMYhJdebNyfb6ybrL3z3HrOqcgBpkKx/GWf9xTYzkzRXHTiQXNcxeED274t7j/XHeaLiOYA0SNa/jP3F1l3W74t7jy3Pa76XwwGkQbL+Zewvtu6yfl/q3HusKM4TlcNL2jZI1suatr7AsuptNC7yeF+asvTusJwnKoe78TaIu4PauJr+8HTXH0dTq6aY3zBffIHGjLvxmptCbGy5ObUcroGY2VjIcvCmLdSrBuIAYmYD8Rd187gJy2wAeU8tX1fuLmvtHEDMOhQxtXxdubustXMAsbGRVa2hiKnl68rdZa2dA4iNhSxrDUVMLV9Xnn3A2jmAWCmyzjFkWWuY7PFd2Gv7MLKct6nIOaDcXdbaOYBY4fLIMWRZa8h7avksE9FFJ7U9lsjauRuvFS6PxaOyfsy5uaT2smdPUvPYtCm72YGzHDVd1xHY7gpcL5XsxivpdEl3SNol6YIu+98j6Zb0slPS45IOS/fNS9qR7nNUqJE8cgxZ1xrynFo+y0R0HZPa7go8PkoLIJJWAh8DzgCOBc6WdGz7MRHxFxFxQkScALwX+N8R8dO2Q05N9y+KjFZdeeQY6rQgVZaJ6Domtd0VeHyUWQM5CdgVEXdHxCPAFcD6JY4/G7i8kJJZrvLKMdRlQaosE9F1TGpnWWvyIlLlKjOAHAnc03Z/b7ptEUkTwOnA59o2B/A1STdJmu31JJJmJW2TtG3fvn0ZFNtGVafaQh6yTETXMamdVa3JTWHlKy2JLun3gN+OiLel988BToqId3Q59l8DfxARr2/b9pyIuFfSM4HrgHdExDeXek4n0c3Kl9WyAnXtQFBHVUyi7wWObrt/FHBvj2PPoqP5KiLuTa8fAL5A0iRm5Futr2KTgeetqpesak117EAwbsoMIDcCayQdI+lgkiBxVedBklYBrwaubNv2FEmHtG4DrwN2FlLqisuzWl/FJgPPWzW6MgLwzHEzzG+Y58CFB5jfMD9Uk1sdOxCMm9ICSEQ8BpwPXAvcDnw2Im6TdJ6k89oOfRPwtYj4f23bngV8S9KtwPeBL0fEV4sq+6jy/BWfZw+XKvae8bxVo6lzAK5jB4Jx44GEBct7WdkV71tBsPhvKsSBCw9U9rGHtWJF8sXXSUp6Y9nS6jQAs+vzeUBiIaqYA2mkvH/F51mtr2KTQRHzVo2zLAd1llGbyaIpbFRVzAsWxQGkYHkn/vKs1lexySDveavGXZYBuInNiVXMCxbJAaRgef+Kz3NcQBXHHMzMwLnnwsqVyf2VK5P7TRlTMqosA3BVp8Gva86xDpwDKVjeOZCBy1Nwm3XWWs0m7b98JyaaNTBxVFl9BvKYJHNUdc45VolzIBVRpV/xde6B09LEZpOsZTUFTBWbE+ucc6wD10AarIq/GAflXliDy7PnUtVqtHnXEKrWopAX10Bskaq2WQ/CvbAGk3fSt2oTWtY551gHDiA1k+Wo4XH48q1is0mVjUvSt9/EeBE9B6vQlbgsDiA1knXOYhy+fJs+s++gxmH+qEFqUU2vIeTNOZAaySNnUbU26yYr4m/RdQbb7Wez8voPcOBnR9XiM+BZeIvXKwfiAFIjThiPr6K6Iy9K+m4/G66+FB59Sq7Pm6WmdJ2tEifRx8A45Cysu6K6I3c26ay8/gMLgkdez5ulpnedrRIHkBoZh5xFkZbqcFC1NUSK7BHXnvQ98LOjCnverFRxSp2mcgCpESeM+7dUh4MqDqDsVYs87LB8A10da7VOjFeHcyA2lpbqcADVG0DZLQdy8MFJgHv00Se2ZZ2f8FQw1g/nQKxRlmoSquIAym61y0MOWRg8oHt+YpTmONdqbRSugdhYqlsNpJt+et25BmFFqGQNRNLpku6QtEvSBV32nyJpv6Rb0suf9nuuNdtSHQ7q0hmhn/yEJ5PMV5MXi+pLRJRyAVYCPwSeCxwM3Aoc23HMKcCXhjm322Xt2rVhzbFlS8TUVISUXG/Z0t++qtiyJWJiIiKphySXiYmFZZUW7m9dpPLKPS62bN8SE5smgov41WVi00Rs2V7BD0vOgG3R5Tu1zBrIScCuiLg7Ih4BrgDWF3CuNcRSE/tVbdK/bvrJT9SxF9VyqtLFelzmDctTmQHkSOCetvt7022dTpZ0q6SvSHrRgOciaVbSNknb9u3bl0W5LUNV+bKoquUCXV2a4/pVpS7W4zBvWN7KDCDqsq0zZXgzMBURLwH+CvjiAOcmGyM2R8S6iFi3evXqoQtr2avSl0VdjVsvqirldDzifXllBpC9wNFt948C7m0/ICIeiohfpLevAQ6SdHg/51r1VenLos7q0BzXryp1sfaI9+WVGUBuBNZIOkbSwcBZwFXtB0h6tiSlt08iKe9P+jnXqq9KXxZWDWXkdHo1o3rE+/KeVNYTR8Rjks4HriXpVfXpiLhN0nnp/r8G/hXwHyQ9BvwSOCvtEdD13FJeiA1tcrL7eIw6J4BtNJs2dR/XkldOp3McTasZFZKa3MxxMw4YS/BAQiuNB8FZN0WuUZPHGjvjyOuB4ABSRV7QysrkNXb60yuAlNaEZQZpM4EDhpXEzaij8WSKZgPwuJXxMm7jaIrmAGLWJ49bGT/jNo6maM6BmPXJCVdrqkrOxmtWJx63YraQA4jVSpk5iHGcuNDKV+cp4x1ArDbKzkE44WpZm9sxx+zVs+zev5sg2L1/N7NXz9YmiDiAWG2UPXeWE66WtbpPGe8AUnHuNvqEKuQgxmniQitf3aeMXzaASDqmn22WvbKbbKrGOQgbN3WfMr6fGsjnumz7n1kXpKrKTHCV3WQzqLxrS85B2Lip+5TxPacykfQC4EXAKkn/sm3X04An512wKmgluFptlK0EF1DIDJ1VaLLp13Kzmmah9TieO8vGRet7ZOPWjezZv4fJVZNsOm1TbWYA7jmQUNJ64I3AG1i41sbPgSsi4jv5Fy9bgw4knP7wNLv3Lx45NrVqivkN813Pmdsxl9mHoU4D1+pUVjMbzMCTKUbElcCVkk6OiO/mWrqKGjTBlXWNpei1EUZRp9qSmWWjnxzImyQ9TdJBkrZK+rGkP8i9ZBUwaIIr6y55deo26gS3NVWdBwKOqp8A8rqIeAj4XZK1yH8deE+upaqIQRNceXTJq0u3USe4LS9V7spe94GAo+ongByUXp8JXB4RP83qySWdLukOSbskXdBl/4yk7enlO5Je0rZvXtIOSbdIymWGxEHXRK57l7xR1Km2ZPVR9a7sdR8IOKplZ+OVdDFJMv2XwEnAocCXIuKlIz2xtBK4E3gtSc3mRuDsiPhB2zEvB26PiAclnQFc1HpeSfPAuoj4cb/PmfdsvJ05EEhqLEsFHTPrreqdM1a8bwXB4u9QIQ5cOD5LGg49G29EXACcTPJl/SjwMLA+gzKdBOyKiLsj4hHgis7HjYjvRMSD6d0bgKMyeN7cDFpjMbOlVb1zRpNbHaC/kegTwNuBT6SbngMsikRDOBK4p+3+3nRbL28FvtJ2P4CvSbpJ0mwG5cnEzHEzzG+Y58CFB5jfMO/gYTaCqnfOqPtAwFH1kwP578AjwMvT+3uB/5rBc6vLtq7taZJOJQkgf9K2+RURcSJwBvB2Sa/qce6spG2Stu3bt2/UMjdKlZOX1gxV75zR+FaHiFjyAmxLr/++bduty53Xx+OeDFzbdv+9wHu7HHc88EPg15d4rIuAdy/3nGvXro0m2rIlYmoqQkqut2zp75yJiYgkdZlcJib6O9csS8N8fi1brTjQeemnBvKIpF8jrR1Ieh7wzxnErhuBNZKOkXQwcBYLR7wjaRL4PHBORNzZtv0pkg5p3QZeB+zMoExjZ9heLHWbh8vGV126sjdRPwHkIuCrwNGS5oCtLGxKGkpEPAacD1wL3A58NiJuk3SepPPSw/4UeAbw8Y7uus8CviXpVuD7wJcj4qujlmkcDRsIqp68NLPyLduNF0DSM4CXkeQtbogBus5WSd7deKtoxYqk5tFJSn7R9VL17pNmVpyhu/FK2hoRP4mIL0fElyLix5K25lNMy9qwvViqmrx0Yt+sOnoGEElPlnQYcLikp0s6LL1Mk3TltRoYNhBUcWR51UclW3n8w6IcS03n/i5gA0mw+BFPdLt9CLg0Iv5bISXMUBObsCD5ZxqHNTTcrGbddK5FA8mPpLJ/8IyTXk1Y/Uxl8o6I+KvcSlagpgaQcTFsPsfGm39Y5G+UqUzGInhY/VV9VLKVwz0Gy9NPN16zSqhqYt/K5R8W5XEAsdqoYmLfyucfFuXpuaStpBOXOjEibs6+OGZLm5lxwLCFWp+Hcegokoe5HXNs3LqRPfv3MLlqkk2nbcpsrq6lemFdn958Msnsu7eS9MQ6HvheRLwykxIUyEl0M2uSrNYoGjiJHhGnRsSpwG7gxIhYFxFrgd8Adg3yIszMbDBZrLWe94qJPZuw2rwgIna07kTETkknZPLsZma2SGfNobXWOjBQzWHP/u5d0XptH1Q/SfTbJX1S0imSXi3pUpLJD83MLAdZ1RzyXjGxnwDyb4DbgNbI9B+k28zMLAdZ1RzyXjFx2SasiPgn4JL0YmZmOZtcNcnu/YuH1w9ac2g1d+XVC6uf2XhfIek6SXdKurt1yeTZGyqL5FiZPHGdWb6yrDnMHDfD/IZ5Dlx4gPkN85kut9tPE9angA8BrwT+RdvFhtBKju3ev5sgfpUcq0sQyWJGXAcgs6XVZa31fiZT/F5EvLSg8uSqCuNApj883bVqOrVqivkN88UXaECjTlznmVPN6mfoyRSB6yX9haSTJZ3YuuRQxkbIKjlW1q/4USeu81rrzeGa5mDq2LTdzziQVu2jPfoE8FujPrmk04GPACuBT0bExR37le4/E3gYeEtrCpXlzq2qLJJjnb/iW81IkP+v+MnJ7jWQfieu88ypzVDmZ7SOshr3UbR+pnM/tcsli+CxEvgYcAZwLHC2pGM7DjsDWJNeZoFPDHBuJWWRHCvzV/yoE9d55tRmcE1zMHmPGM/LkgFE0gsknSbpqR3bT8/guU8CdkXE3RHxCHAFsL7jmPXAZyJxA3CopCP6PLeSskiOlfkrftQZcYcNQG4OqRfXNAeT94jxvCw1G+87gbeTjDr/lKR3RcSV6e73A18d8bmPBO5pu7+XJ5rLljrmyD7PBUDSLEnthcmK/MydOW5mpGrpqM1IoxplRtxhZk51c0j9lP0ZrZusxn0UbakayL8D1kbEG4FTgP+SrpMOT6yPPopuj9HZJazXMf2cm2yM2JxOBLlu9erVAxaxmuq+/sHMTNJj68CB5Hq5IODmkPqp+2e0aHmPGM/LUgFkZUT8AiAi5kmCyBmSPkQ2AWQvcHTb/aOAe/s8pp9zx1bTFlZyc0j9NO0zOqq6jPvotNR6IN8A/mNE3NK27UnAp4GZiFg50hMnj3UncBrwI+BG4Pcj4ra2Y34HOJ+kF9ZLgY9GxEn9nNtNFcaB2BP6Xehm1LEnZjaaYcaBvBn4x/YNEfFYRLwZeNWoBYqIx0iCw7UkeZbPRsRtks6TdF562DXA3STrj1wK/OFS545apqqrYz/xXgYZke/mELNqWnYk+jipcw0kq5XFqmLQEflzc16y1Kwso4xEtxHVYWWxog3abXHQxLtZ2capxaCXfkai2wjqsrJY0erabdGsH3UdWT4o10ByVpeVxYpW126LZv0YtxaDXhxAclaXlcWKVtdui2b9GLcWg17chJWzuqwsVoZRR+SbVVVTmmhdA8lZXVYWa0LCz6wo49Zi0IsDSM7q0FRT91USzVqqMulmHf7vs+BxIFb7VRLNwKtd5snjQKynpiT8ilaVX8NN4Uk3i+cAYmPXRbgKWr+Gd++GiCemoHcQyY8n3SyeA4g1JuG3nCxrDP41vFARtbFRV7t0jXEIEdGYy9q1a8O627J9S0xdMhW6SDF1yVRs2b4ln+fZEjE1FSEl11vyeZqBbdkSMTERkdQXksvExPDlkxY+VusiZVvuOsj6vc3jeYoqY10B26LLd2rpX+pFXhxAylXlf9Kpqe5f+FNT1Xi8OivyvRj2B4r/XkvrFUDchFUh4z4WY9BmnSKbFLJuP/cU9E8oMjcx7KSbRZZxnP7PHUAqogljMQb5Jy06CT1q+3knr8j3hKzf2zwUVcZx+z93AKmIJky+Nsg/adFJ6DxqDFWcgr6MRHEdamNFlXHc/s8dQCqiCWMxBvknLbpLZhNqDGV1La7De1tUGcft/7yUACLpMEnXSborvX56l2OOlnS9pNsl3SbpXW37LpL0I0m3pJczi30F2WvCWIxB/knLaPaoYo0hS2V2LS7ivR21dlVEGcft/7ysGsgFwNaIWANsTe93egz4TxHxQuBlwNslHdu2/5KIOCG9XJN/kQczaKKsbmMxhk0E9vtPWodmj7oZ54F2dRm4Wbf/8+WUFUDWA5elty8D3th5QETcFxE3p7d/DtwOHFlYCUcwTKKsTpOvFZEIrEOzR93UIZk9rLoM3KzT/3k/SplMUdLPIuLQtvsPRsSiZqy2/dPAN4EXR8RDki4C3gI8BGwjqak82OPcWWAWYHJycu3u3YsnDczaOE5OODeX/DPu2QMrDt3L46f+MRx/+YJjqvb62ss8OZnUXpocgMZ5ssEVK5KaRycpqe3aaAqfTFHS1yXt7HJZP+DjPBX4HLAhIh5KN38CeB5wAnAf8MFe50fE5ohYFxHrVq9ePeSrGcy4Jco6mwcef/AouPpS2H72guOq9Prq0qRRpHGu1Y1z7arKcgsgEfGaiHhxl8uVwP2SjgBIrx/o9hiSDiIJHnMR8fm2x74/Ih6PiAPApcBJeb2OYYxboqxb8wCPPgW2vn/Bpiq9vro0aRRtXDsKOGdWjrJyIFcB56a3zwWu7DxAkoBPAbdHxIc69h3RdvdNwM6cyjmUuiTK+k2E90yy7n8iYFTt9Y1zwtgWG+faVZWVFUAuBl4r6S7gtel9JD1HUqtH1SuAc4Df6tJd9wOSdkjaDpwK/FHB5V9SHRJlgyTCezUDrHz6vZV9fW7SaJ5xrV1VmVckbKhBEv11TL7WscxmVeUVCW2BQRL9dWweqGOZzerGNZCGGseuxmaWD9dAbIG6JPrNrLocQBqqDol+M6s2B5AGmzluhvkN8xy48ADzG+bHJniM04I9Vj9N+vw9qewCmGWp1T25teZCq3syMDYB0qqraZ8/10BsrIzbgj1WL037/DmA2FhZqntyHqvxlbHCn1XXuM2DtxwHEOtbHdp2e83Hddhd52c+uWKVJmx0IKuGcZsHbzkOIDVUxhd5EWuAZKFX92S+8f7MJ1esyoSNVQpki8pWgx8dWWpa93gPJKyZziQdJB/QvLvg1mng4dyOOTZu3cie/XuYXDXJptM2cc5LZjJfL6Iqa1BMTydBo9PUVDInVFnK+qyWrdvnr+6vt9dAQgeQminri3zF+1YQLP6sCHHgwuqv2JPHl2xVvrirEsg61elHhy3NI9HHRFlJurq27baaUHavnUEHLWxvGnW9iKqsQVHVmYebllBuIgeQminri7yObbvteRuO/zvi9W9Dh+4BRSaTK1ZlwsaqBLJOdf3RYf1zAKmZsr7I6zj1yaI++cdfTmyYYupDx2S2XkQV1qCoSiDrVMcfHTYY50BqaByTdHmoe95mHPizOh6cRGd8Aoj1x0lcs2xUKoku6TBJ10m6K71+eo/j5tOla2+RtG3Q863Z3IRilq+yciAXAFsjYg2wNb3fy6kRcUJH9BvkfGuoOuZtzOqklCYsSXcAp0TEfZKOAP5XRDy/y3HzwLqI+PEw53dyE5ZV1dxcMoJ9z56k++2mTeUnwc1aKtWEBTwrIu4DSK+f2eO4AL4m6SZJs0Ocb1Z5VZ6KpCh1n8uraVO2tOQWQCR9XdLOLpf1AzzMKyLiROAM4O2SXjVEOWYlbZO0bd++fYOebpa7qsyplYd+AkPeATTv4FSXeeLyUOkmrI5zLgJ+ERF/6SYsGydVnYpkVK3A0B4cJyYWj1HJc0qYfsswiib09qtaE9ZVwLnp7XOBKzsPkPQUSYe0bgOvA3b2e75ZXVR1KpJR9Vuz2tNjZpNe2/MowyiaPGVLWQHkYuC1ku4CXpveR9JzJF2THvMs4FuSbgW+D3w5Ir661PlmdVTVqUg6DdoU1G9gyDOAZh2cuuU6mjxlSykBJCJ+EhGnRcSa9Pqn6fZ7I+LM9PbdEfGS9PKiiNi03PlmdVTVqUjaDZOn6Dcw5BlAswxOvXIdZ645s7HjjTwXllkXRfeqaZ9Ta9PVc2zcV16Pnm6vfZimoH4DQ54BNMvg1Gu982vuuqax4408lYlZhzIXQip7EaZez//wf/4FhBYdv1yif7nxLUXMlZXVGJsmz63mubBwALH+lNmrpp/nznPQYa/nX/mRe3j8waMWl2uEnlJlB8tBNaG3VS9V64VlVlll9qpZ7rnzHjPR6/kfP/VPMs9T9GoS2ri1mgNgPLfaYg4gZh3K7FWz3HPn3S211/NP/ea3M89T1K37q+dWW8wBxKxDmb80l3vuPMdMLPf8WS+eVcfurzPHzTC/YZ4DFx5gfsN8o4MHOIBYhZU1v1CZvzSXe+68Bx0W+drdJFR/TqJbJdUtwVqUIqbmKJJXLKwH98LCAaROmtzjZTme+t2K1iuAPKmMwpgtp24J1iLNzDhgWDU4B2KVVMcEq1nTOIBYJTnBalZ9DiBWSe5zb1Z9TqKbmdmSPJWJmZllygHEzMyG4gBiZmZDcQAxs9ooa3ob666UACLpMEnXSborvX56l2OeL+mWtstDkjak+y6S9KO2fWcW/yrMBuMvv9H0WlLW72N5yqqBXABsjYg1wNb0/gIRcUdEnBARJwBrgYeBL7Qdcklrf0RcU0ipzYbkL7/R1W39kCYoK4CsBy5Lb18GvHGZ408DfhgRiydHMquBKnz51b0G5OltqqesAPKsiLgPIL1+5jLHnwVc3rHtfEnbJX26WxNYi6RZSdskbdu3b99opTYbUtlffuNQA/L0NtWTWwCR9HVJO7tc1g/4OAcDbwD+R9vmTwDPA04A7gM+2Ov8iNgcEesiYt3q1auHeCVmoyv7y68KNaBReXqb6sktgETEayLixV0uVwL3SzoCIL1+YImHOgO4OSLub3vs+yPi8Yg4AFwKnJTX6zDLQtlffmXXgLLg6W2qp6zp3K8CzgUuTq+vXOLYs+lovpJ0RKsJDHgTsDOPQpplpfUlV9biSZOrJruur1K35p+Z42YcMCqklLmwJD0D+CwwCewBfi8ifirpOcAnI+LM9LgJ4B7guRGxv+38vyVpvgpgHvj3bQGlJ8+FZU3lFR5tFJVaUCoifkLSs6pz+73AmW33Hwae0eW4c3ItoNmYKbsGZOPJs/GamdmSPBuvmZllygHEzMyG4gBiZmZDcQAxM7OhOICYmdlQGtULS9I+oMgJGQ8Hflzg8w3K5RuNyzcal280RZZvKiIWzQXVqABSNEnbunV9qwqXbzQu32hcvtFUoXxuwjIzs6E4gJiZ2VAcQPK1uewCLMPlG43LNxqXbzSll885EDMzG4prIGZmNhQHEDMzG4oDyIgkHSbpOkl3pdeL1meX9HxJt7RdHpK0Id13kaQfte07c/Gz5Fu+9Lh5STvSMmwb9Pw8yyfpaEnXS7pd0m2S3tW2L5f3T9Lpku6QtEvSBV32S9JH0/3bJZ3Y78IYt0QAAAXVSURBVLkFlW8mLdd2Sd+R9JK2fV3/1gWX7xRJ+9v+bn/a77kFle89bWXbKelxSYel+3J9/yR9WtIDkroulFf2Z2+BiPBlhAvwAeCC9PYFwJ8vc/xK4B9JBuYAXAS8u+zykSzMdfiory+P8gFHACemtw8B7gSOzev9S/9GPwSeCxwM3Np6vrZjzgS+Agh4GfC9fs8tqHwvB56e3j6jVb6l/tYFl+8U4EvDnFtE+TqOfz3wjQLfv1cBJwI7e+wv7bPXeXENZHTrgcvS25cBb1zm+NOAH0ZEUSPiBy1f1ueP/PgRcV9E3Jze/jlwO3BkxuVodxKwKyLujohHgCvScrZbD3wmEjcAh0o6os9zcy9fRHwnIh5M794AHJVxGUYqX07n5lW+Rctq5ykivgn8dIlDyvzsLeAAMrpnRbqcbnr9zGWOP4vFH8bz06rop7NuIhqgfAF8TdJNkmaHOD/v8gEgaRr4DeB7bZuzfv+OJFlKuWUviwNWr2P6ObeI8rV7K8kv1pZef+uiy3eypFslfUXSiwY8t4jytZbVPh34XNvmvN+/5ZT52VuglCVt60bS14Fnd9m1ccDHORh4A/Dets2fAP6M5EP5Z8AHgX9bQvleERH3SnomcJ2kf0h/CY0sw/fvqST/yBsi4qF088jvX7en6rKts797r2P6OXdUfT+HpFNJAsgr2zbn9rceoHw3kzTj/iLNW30RWNPnuUWUr+X1wLcjor1GkPf7t5wyP3sLOID0ISJe02ufpPslHRER96XVyAeWeKgzgJsj4v62x/7VbUmXAl8qo3yRrEdPRDwg6Qsk1eFvAoO8vtzKJ+kgkuAxFxGfb3vskd+/LvYCR7fdPwq4t89jDu7j3CLKh6TjgU8CZ0TET1rbl/hbF1a+th8ARMQ1kj4u6fB+zi2ifG0WtRgU8P4tp8zP3gJuwhrdVcC56e1zgSuXOHZRW2r6pdnyJqBrz4sRLFs+SU+RdEjrNvC6tnIM8vryKp+ATwG3R8SHOvbl8f7dCKyRdExaazwrLWdnud+c9oh5GbA/bYLr59zcyydpEvg8cE5E3Nm2fam/dZHle3b6d0XSSSTfRT/p59wiypeWaxXwato+kwW9f8sp87O3UJ4Z+iZcgGcAW4G70uvD0u3PAa5pO26C5B9kVcf5fwvsALanf+wjii4fSa+NW9PLbcDG5c4vuHyvJKmKbwduSS9n5vn+kfR0uZOkV8vGdNt5wHnpbQEfS/fvANYtdW4On7vlyvdJ4MG292vbcn/rgst3fvr8t5Ik+V9epfcvvf8W4IqO83J//0h+ZN4HPEpS23hrlT577RdPZWJmZkNxE5aZmQ3FAcTMzIbiAGJmZkNxADEzs6E4gJiZ2VAcQMxGsNzMqR3HniLp5YPu6+NxpyX9/jDnmo3CAcRsNH9DMldSP04hmSV30H3LmQYcQKxwHgdiNqJ0gscvRcSL27a9k2Tw12PAD0imqr8BeBzYB7wjIv5P2/kL9gH/APw1MJk+5IaI+LakVwMfSbcFydTf1wEvBP4vcFlEXJLPKzVbyAHEbEQ9Asi9wDER8c+SDo2In0m6CPhFRPxll8dYsE/S3wEfj4hvpdOSXBsRL5R0NXBxGkyeCvwTyUj9d0fE7+b7Ss0W8mSKZvnYDsxJ+iLJTLODeg1wbDpdFMDT0jmYvg18SNIc8PmI2Nt2jFmhnAMxy8fvkMxXtBa4SdKgP9ZWACdHxAnp5ciI+HlEXAy8Dfg14AZJL8i22Gb9cwAxy5ikFcDREXE98MfAocBTgZ+TLMnbTee+r5FMONh6zBPS6+dFxI6I+HNgG/CCZR7XLDcOIGYjkHQ58F3g+ZL2SnorydrUWyTtAP4euCQifgZcDbxJ0i2SfrPjoTr3vRNYp2SlxR+QJOQBNkjaKelW4JckKw1uBx5TsrrfH+X9ms1anEQ3M7OhuAZiZmZDcQAxM7OhOICYmdlQHEDMzGwoDiBmZjYUBxAzMxuKA4iZmQ3l/wPCJ4J+xwqVwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from plotutil import plotData\n",
    "\n",
    "data = np.loadtxt('pa3-data2-polynomial-train.csv', delimiter=',')\n",
    "X_train = data[:,:-1].T\n",
    "y_train = data[:,-1].T\n",
    "\n",
    "#plot data\n",
    "plotData(None, None, X_train, y_train, \"1st test\", \"2nd test\")\n",
    "\n",
    "data_test = np.loadtxt('pa3-data2-polynomial-test.csv', delimiter=',')\n",
    "X_test = data_test[:, :-1].T\n",
    "y_test = data_test[:, -1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = Run_Experiment(X_train, y_train, X_test, y_test, epochs = 10000, learning_rate = 0.02, lmd = 1.5, print_loss = True)\n",
    "# Plot learning curve (with costs)\n",
    "losses = np.squeeze(d['losses'])\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(d['model'], lambda x: normalize(x), X_train, y_train, xlabel=\"1st test\", ylabel=\"2nd test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
